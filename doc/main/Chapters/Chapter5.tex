% Chapter 1

\chapter{Results and Analysis} % Main chapter title

\label{Chapter5} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 5. \emph{Results and Analysis}} % This is for the header on each page - perhaps a shortened title

\emph{In this section we present the results of our experimentation with feature engineering. We notably begin with a comparison between GROBID and refextract, the existing partial solution for metadata extraction at INSPIRE-HEP. Following this we detail the evaluation method and our approach to running experiments. Finally, we present the results of our experimentation and provide our analysis and interpretations.}

\section{Comparison with \emph{refextract}}

As a first result for GROBID, we compare it with \emph{refextract}, the existing solution for automatic reference extraction at CERN. \emph{refextract} is an example of a \emph{stylistic analysis} tool, as it employs regular expressions in a heuristic framework for extraction. As previously mentioned, \emph{refextract} is incomplete and greatly lacking in both breadth and depth of detail. It is capable only of retrieving references\footnote{A comparatively easy task; GROBID's citation model usually performs at a significantly higher accuracy than, say, its \emph{header} model.}, and the classification itself is quite basic. Since the modelling of reference fields differs between the two, a comparison is difficult to make. Our results will at least be indicative, however, and we are able to make reasonable comparisons on the most important fields. The dataset for the comparison consists of 60 articles coming from the SCOAP$^3$ online repository\footnote{Scoap$^3$ (Sponsoring Open Consortium for Open Access Publishing in Particle Physics) is an open access digital library hosted at CERN, backed by an international partnership of research institutions.}.

Unlike \emph{refextract}, GROBID requires two separate models to classify the citations of a given article: the \emph{reference-segmenter} and \emph{citation} models\footnote{Strictly speaking, there is another model, (full) \emph{segmentation}, above the \emph{reference-segmenter}, and so \emph{citation} accuracy depends on this also. But because one focus of our work is to improve this model, we accept this omission.}. The \emph{reference-segmenter} model is the simplest model in GROBID's arsenal, and is responsible for segmenting a reference list block into individual references. Therefore, the accuracy of the citation model is ultimately subject to the accuracy of the reference block inputs supplied to it by the \emph{reference-segmenter} above. The results for training and evaluating the \emph{reference-segmenter} on 60 SCOAP$^3$ papers with an 80--20 split are given in Table \ref{table:referencesegmenterresults}. The results show the \emph{reference-segmenter} is extremely accurate. In fact, only 5 token misclassifications out of 622 were made for the <label> class, and from a grand total of 12,981.

\label{subsec:refextract}
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|cccc|}
\hline
label           & accuracy  & precision  & recall   & f1 \\
\hline
<label>         & 99.96     & 100        & 99.2     & 99.6\\
<reference>     & 99.96     & 99.96      & 100      & 99.98\\
\hline
(micro average) & 99.96     & 99.96      & 99.96    & 99.96  \\
(macro average) & 99.96     & 99.98      & 99.6     & 99.79  \\
\hline
\end{tabular}
\caption[Evaluation results for reference segmentation]{Evaluation results for reference segmentation}
\label{table:referencesegmenterresults}
\end{center}
\end{table}

The most significant difference between the tools is the set of classes modelled. \emph{refextract} attempts only to classify to the minimum detail required for identifying the originating document within the digital library. Therefore, there are no equivalents to GROBID's classes, <volume>, <pages>, and so on. Rather, these parts of references are absorbed into other, higher-level classes, and are indicated by a dash (-) in the results table. Comparisons can be made, however on fields, <title>, <author>, <journal>, and <date>. There we see the superiority of GROBID over \emph{refextract}. Note that here the \emph{citation} was not trained on the evaluation set, and in particular this may explain its dismal performance in \emph{date} precision, and \emph{pubnum} recall. The dataset instances contained a recurring publication number that was almost uniformly misclassified by GROBID as a date. Notice that this is an example of a domain specificity of HEP papers. Had we trained on these papers, we could expect an improvement.

That \emph{refextract} has perfect recall in the given fields is due to the way the evaluation was conducted, namely...

For an explanation of the performance metrics, see Section \ref{sec:evaluation}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|cccc|cccc|}
\hline
engine &  \multicolumn{4}{|c|}{GROBID} & \multicolumn{4}{c|}{\emph{refextract}}\\
\hline
label & accuracy & precision & recall & f1 & acc. & prec. & rec. & f1\\
\hline
<author>    & 99.85 &   99.68   &   99.75   &   99.72   & 98.33 &   100 &   92.22   &   95.95   \\
<title> &   99.59   &   98.87   &   99.25   &   99.06   & 94.89 &   100 &   71.75   &   83.55   \\
<journal>   & 98.84 &   88.87   &   93.98   &   91.35   & 97.12 &   100 &   46.78   &   63.74   \\
<volume>&   99.95   &   99.07   &   98.15   &   98.6    & -     &   -   &   -       &   -   \\
<issue> &   99.93   &   100     &   94.63   &   97.24   & -     &   -   &   -       &   -   \\
<pages> &   99.75   &   93.51   &   99.45   &   96.39   & -     &   -   &   -       &   -   \\
<date>  &   98.39   &   57.39   &   98.31   &   72.47   & 98.88 &   100 &   37.55   &   54.6    \\
<pubnum>&   98.71   &   100     &   12.96   &   22.95   & -     &   -   &   -       &   -   \\
<note>  &   99.4    &   43.75   &   35      &   38.89   & -     &   -   &   -       &   -   \\
<publisher>&99.81   &   63.46   &   94.29   &   75.86   & -     &   -   &   -       &   -   \\
<location>& 99.81   &   86.32   &   91.11   &   88.65   & -     &   -   &   -       &   -   \\
<institution>& 99.78&   25      &   25      &   25      & -     &   -   &   -       &   -   \\
<booktitle>&    98.7&   55.56   &   41.67   &   47.62   & -     &   -   &   -       &   -   \\
<web>   &   99.64   &   51.85   &   100     &   68.29   & -     &   -   &   -       &   -   \\
<editor>    &  99.93&   100     &46.67      &   63.64   & -     &   -   &   -       &   -   \\
<tech>  &   99.95   &   83.33   &   50      &   62.5    & -     &   -   &   -       &   -   \\
\hline
(micro average) & 99.5  &   93.63   &   94.77   &   94.19 & -    &   - &   -   &   -   \\
(macro average) & 99.5  &   77.92   &   73.76   &   71.76 & -    &   -  &   -   &   -   \\
\hline
\end{tabular}
\caption[Evaluation results for citations]{Evaluation results for citations}
\label{table:citationcomparison}
\end{center}
\end{table}

\section{Experiment Setup}
\label{sec:experimentsetup}
Here talk about k-fold, number of iterations, computing resources etc.

\section{Evaluation Method}

\begin{table}[h]
\begin{center}
\begin{tabular}{ | p{0.2\linewidth} | p{0.25\linewidth} | p{0.15\linewidth} | p{0.3\linewidth} |}
\hline
Feature Category & Variations & Models & Data\\
\hline
Baseline & - & Segmentation, header & CORA, CORA app. HEP, CORA + HEP, HEP, HEP app. CORA \\
\hline
Baseline & - & Header & HEP app. 1/3 CORA, HEP app. 2/3 CORA \\
\hline
Dictionaries & First order, Second order, Third order & Header & HEP app. 1/3 CORA, HEP app. 2/3 CORA \\
\hline
Dicts. + Stops & First order, Second order, Third order, Stops only & Header & HEP app. 1/3 CORA, HEP app. 2/3 CORA \\
\hline
Regularisation & $\sigma=0$, $\sigma=\exp\{-6\}$,$\sigma=\exp\{-5\}$,$\sigma=\exp\{-4\}$,$\sigma=\exp\{-3\}$, & Header & HEP app. 1/3 CORA, HEP app. 2/3 CORA \\
\hline
Token Extension & +5, +10, +15, +20 & Header & HEP app. 1/3 CORA, HEP app. 2/3 CORA \\
\hline
Block Size & Height, Width, Height \& Width, Area & Segmentation, Header & HEP \\
\hline
Levenshtein & $T_1 = 0.05$, Lev$\geq0.1$, Lev$\geq0.2$, Lev$\geq0.4$, Lev$\geq0.8$, Lev$\geq0.1 \& \geq0.4$ LevAll & Header & HEP \\
\hline
Character Classes & Binary, Decimal, Decimal only & Segmentation & HEP \\
\hline
\end{tabular}
\caption[Character classes used as features, along with the regular expressions used to count them.]{Character classes used as features, along with the regular expressions used to count them.}
\label{table:experiments}
\end{center}
\end{table}

\section{Baseline}

\section{Block Size}

\section{Character Classes}

\section{Dictionaries}

\section{Dictionaries + Stop Words}

\section{Levenshtein}

\section{Regularisation}

\section{Token Selection}