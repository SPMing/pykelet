% Chapter 6

\chapter{Conclusion} % Main chapter title

\label{Chapter6} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 6. \emph{Conclusion}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

\section{Summary}

Our overarching hypothesis that HEP papers are a special case and have qualitative differences to conventional scientific papers. We demonstrated this in a number of ways, first through a qualitative inspection of HEP papers in Chapter \ref{Chapter4}, then quantitavely, through our experimentation in Chapter \ref{Chapter5}, such as through subsampling, which defied the conventional wisdom of expanding trainging data, indicating a problem in generalising over both HEP and CORA papers. Subsequent to this, our approach was:

\begin{enumerate}
\item to train models on a custom HEP training set, and;
\item to engineer specialised features conducive to better generalisation on HEP papers.
\end{enumerate}

Compiling a custom training set required much tedious manual work, as it had to be done for both \emph{header} and \emph{segmentation} models.  One positive outcome of the data acquisition, however, was that we more than doubled the existing training set for the \emph{segmentation} model. There may also be value in the 60 SCOAP\textsuperscript{3} papers that we configured for training in our successful comparison of GROBID with \emph{refextract} (Section \ref{sec:refextract}). This also increases the \emph{reference-segmenter} model training set substantially.

Our feature engineering ideas were based on trying domain-specific features, such as INSPIRE-HEP derived dictionary features for the \emph{header} model, or by addressing the structural characteristics of the papers in the \emph{segmentation} model. Of particular success were the features based on the character class composition for line tokens in the \emph{segmentation} model. This gave our best overall results, proving to be highly effective indicators for differentiating between token classes. Levenshtein distance between lines, which functioned as a single categorical variable, also proved to be effective. These latter features further demonstrated the value in more elaborate and systematic approaches to feature engineering, which in the baseline remains quite simplistic and indicative. 

Our results suggest that the best features are those giving a \emph{dimensionality reduction} to tokens, that is, those features offering a mapping to a lower-dimensional variable. Block shape features were unsuccessful because they did not describe the individual token well, rather, addressed groups of tokens. In contrast, for the \emph{header} model, our best results were for dictionary-based features, which map word tokens to a lower-dimensional group. This was most  for our novel \emph{stop word} dictionary, which achieved an error reduction in F1 of $12\%$, as well as very great error reductions for several key classes. We may note additionally that applying conditional random fields to \emph{line} tokens, such as in the \emph{segmentation} model offers far greater scope for novelty than the header model. Our most significant overall improvement were our character class features, which achieved error reductions to key classes, <header> and <references>, of $24\%$ and $21\%$ respectively. Our Levenshtein distance features are exceptional in that they model differences between successive lines of text. This appears to have resolved many of the transitional misclassifications observed in our data acquisition (Section \ref{sec:data}).

\section{Future Work}
\label{sec:futurework}

We list ideas for future work on this project:

\begin{enumerate}

\item An obvious starting point for improving upone our work is to continue expanding the HEP training sets for each of the models. A major finding of our work is that long-term improvement of the models will require the manual creation of HEP training data. 

\item Another extension to GROBID would be to model collaborations in the \emph{citation} model, as we achieved for the \emph{header} model Section \ref{subsec:extensions}).

\item Not much came of our regularisation experiments other than to reinforce the findings in the literature. It would be interesting to try other forms of regularisation, such as $l_1$. This requires an optimisation algorithm that is not gradient-based, but Wapiti does offer such alternatives to L-BFGS.

\item In terms of experimentation, an obvious missing . With the exception of combining dictionaries and stop words, we did not find a combination of features. Our strongest pair of features for the \emph{segmentation} model did not yield additional gains in combination. Investigations could therefore be made into the interaction of feature categories, and methodology derived for how best to combine them

\item Our work would have been made easier with further enhancements to GROBID's evaluation utilities. For example it would be useful to see exactly which tokens are being misclassified when misclassificationc occur. We went some way towards this in \ref{subsec:extensions} where we tracked the $k$ documents most contributing to classification error.

\end{enumerate}

