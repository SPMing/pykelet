
===== Token-level results =====


label		accuracy	precision	recall		f1

<title>		99.25		91.02		90.64		90.83
<author>		99.23		87.4		95.91		91.46
<email>		99.7		91.17		90.1		90.63
<address>		99.53		93.3		90.88		92.07
<affiliation>	99.38		92.8		92.47		92.64
<phone>		99.94		71.7		56.3		63.07
<abstract>		90.63		86.86		91.84		89.28
<keyword>		99.68		91.49		85.92		88.62
<copyright>		99.68		86.18		69.8		77.13
<note>		96.13		52.98		51.62		52.29
<web>		99.85		87.6		74.7		80.64
<intro>		90.44		68.95		56.59		62.16
<pubnum>		99.77		84.15		85.2		84.67
<date>		99.88		88.48		81.76		84.99
<reference>		99.17		72.48		82.14		77.01
<submission>	99.79		86.56		84.44		85.49
<grant>		99.67		85.84		79.46		82.52
<date-submission>	99.96		5.17		100		9.84
<entitle>		99.99		0		0		0
<degree>		99.35		23.7		26.77		25.15
<dedication>	99.99		100		51.61		68.09
<collaboration>	100		57.14		100		72.73

all fields		98.68		83.04		83.04 		83.04	(micro average)
			98.68		72.95		74.46		70.97	(macro average)

===== Field-level results =====

label		accuracy	precision	recall		f1

<title>		97.77		90.25		89.56		89.9
<author>		97.52		88.66		90.88		89.75
<email>		99.34		94.64		93.56		94.1
<address>		98.05		94.5		89.41		91.88
<affiliation>	96.71		90.28		86.23		88.21
<phone>		99.85		81.82		64.29		72
<abstract>		98.22		86.71		89.02		87.85
<keyword>		99.55		91		88.35		89.66
<note>		95.19		64.81		65.42		65.12
<copyright>		99.38		84.62		74.32		79.14
<web>		99.59		76.19		78.05		77.11
<intro>		98.78		75.29		64		69.19
<pubnum>		99.23		84.68		83.19		83.93
<date>		99.27		90.6		87.1		88.82
<reference>		98.61		78.44		81.88		80.12
<submission>	99.42		88.04		83.51		85.71
<grant>		99.74		80.65		80.65		80.65
<date-submission>	99.98		50		100		66.67
<entitle>		99.96		0		0		0
<degree>		99.79		71.43		38.46		50
<dedication>	99.98		100		50		66.67
<collaboration>	99.94		50		100		66.67

all fields		98.9		87.15		85.43 		86.28	(micro average)
			98.9		77.85		76.27		75.6	(macro average)

===== Instance-level results =====

Total expected instances: 		534
Correct instances: 		217
Instance-level recall:	40.64

===== Confusion matrix =====

<title>	5120	16	0	0	8	0	91	0	2	227	1	71	32	1	80	0	0	0	0	0	0	0
<author>	13	5658	0	25	37	0	23	0	0	117	3	2	1	0	16	0	0	0	0	4	0	0
<email>	0	9	2003	11	0	30	4	0	0	163	0	0	3	0	0	0	0	0	0	0	0	0
<address>	0	38	28	3787	163	0	15	0	8	120	0	0	0	1	0	0	0	0	0	8	0	0
<affiliation>	27	86	12	127	5377	0	26	0	0	134	0	0	1	1	7	0	9	0	0	5	0	2
<phone>	0	0	55	0	0	76	0	0	0	4	0	0	0	0	0	0	0	0	0	0	0	0
<abstract>	25	3	1	52	7	0	53745	50	12	433	3	4052	0	0	118	0	18	0	0	0	0	0
<keyword>	9	0	22	24	24	0	88	1709	0	39	0	0	0	0	74	0	0	0	0	0	0	0
<copyright>	0	8	25	0	52	0	11	0	742	200	4	0	13	0	9	0	0	0	0	0	0	0
<note>	301	423	11	11	69	0	248	93	12	2919	12	592	63	19	136	66	151	55	0	469	0	4
<web>	0	0	31	0	0	0	18	0	0	49	431	24	0	0	5	19	0	0	0	0	0	0
<intro>	77	0	3	0	0	0	7581	2	8	321	31	10811	13	7	236	15	0	0	0	0	0	0
<pubnum>	3	4	0	0	1	0	0	14	10	92	0	22	892	0	9	0	0	0	0	0	0	0
<date>	7	2	0	0	1	0	6	0	0	43	7	10	3	484	14	15	0	0	0	0	0	0
<reference>	19	123	0	9	15	0	10	0	21	145	0	21	31	2	1909	19	0	0	0	0	0	0
<submission>	3	5	0	5	5	0	7	0	0	89	0	0	0	25	20	863	0	0	0	0	0	0
<grant>	0	0	6	5	9	0	0	0	0	176	0	75	8	0	0	0	1079	0	0	0	0	0
<date-submission>	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	3	0	0	0	0
<entitle>	0	0	0	0	0	0	0	0	0	13	0	0	0	0	1	0	0	0	0	0	0	0
<degree>	21	99	0	3	26	0	0	0	46	211	0	0	0	7	0	0	0	0	0	151	0	0
<dedication>	0	0	0	0	0	0	0	0	0	15	0	0	0	0	0	0	0	0	0	0	16	0
<collaboration>	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	8


