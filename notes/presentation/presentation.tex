%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

\usetheme{Madrid}

\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{textpos}
\usepackage{subfigure}
\usepackage{cite}

\DeclareMathOperator*{\argmax}{\arg\!\max}

\title[Automatic Metadata Extraction]{Automatic Metadata Extraction with Conditional Random Fields} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Joseph Boyd} % Your name
\institute[EPFL] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
\'Ecole Polytechnique F\'eq\'erale de Lausanne \\ % Your institution for the title page
\medskip
\textit{joseph.boyd@epfl.ch} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

%------------------------------------------------

\section{The Problem}
\begin{frame}[noframenumbering]{Outline}
\tableofcontents[currentsection, currentsubsection]
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{The Problem}
\begin{itemize}
\item Metadata extraction of scientific articles is a difficult and well-studied problem
\item Metadata usually refers to header information (title, authors, affiliations, publisher...), bibliographic information (citations and their sub-components), document structure (sections, etc.)
\item Extraction of metadata refers to processing and then \emph{classifying} text blocks and other components of the document.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The Problem - An Illustration}
\begin{figure}[!t]
\center
\subfigure{\includegraphics[width=2.25in]{figures/header1.pdf} \label{fig:predictionerror}}
%\hfill
%\subfigure{\raisebox{19mm}{\includegraphics[width=0.5in]{Figures/plus.png} \label{fig:predictionerror}}}
%\hfill
\subfigure{\includegraphics[width=2.25in]{figures/header2.pdf} \label{fig:logvsrand}}
\end{figure}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{The Problem - Some Assumptions}
\begin{itemize}
\item We are always dealing with PDF documents, and an OCR tool gives us access to the plaintext and OCR information (font, size, orientation,...)
\item Metadata has structure and is not completely random, but it is infeasible to model this deterministically $\Rightarrow$ the task is inherently error-prone
\item There is no one-size-fits-all model for processing a full document $\Rightarrow$ the problem must be decomposed.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{The Problem - Solutions}
There are a number of approaches to metadata extraction:
\begin{itemize}
\item Template-based
\item Knowledge base (consult online repositories)
\item Machine learning techniques (HMMs, SVMs, CRFs)
\end{itemize}
In practice, tools for metadata extraction combine these approaches (an example of which we will see later) % \cite{comparison}
\end{frame}

%------------------------------------------------

\section{Theory}
\subsection{Hidden Markov Models}

%------------------------------------------------

\begin{frame}[noframenumbering]{Outline}
\tableofcontents[currentsubsection]
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Hidden Markov Models (HMMs)}
\begin{itemize}
\item Hidden Markov Models (HMM) build a probabilistic model on sequential data, 
\item We have a sequence of observations, and we want to predict the ``hidden'' states which generate them
\item The Markov condition postulates a dependence of one state to the next as we move in time
\item HMMs have applications in fields as diverse as text processing, bioinformatics, and artificial intelligence.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
A well-known application of HMMs is from Natural Language Processing (NLP) - Part of Speech (PoS) tagging.
\frametitle{Hidden Markov Models (HMMs) - Example}
\begin{figure}[!ht]
\center
\includegraphics[width=3in]{figures/pos1.pdf}
\end{figure}

\begin{figure}[!ht]
\center
\includegraphics[width=3in]{figures/pos2.png}
\end{figure}
Metadata extraction is a similar problem...
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Hidden Markov Models (HMMs) - Prediction}
\begin{itemize}
\item A Hidden Markov Model factorises in the following way:
$$p(\mathbf{x}, \mathbf{y}) = \prod_{t=1}^T p(y_t | y_{t-1})p(x_t | y_t)$$
\item The probabilities $p(y_t | y_{t-1})$ are ``transition'' probabilities, and $p(x_t | y_t) $``emission'' probabilities, and must be calculated in advance.
\item When it comes to prediction, we use dynamic programming Viterbi algorithm ($\mathcal{O}(T|S|^2))$ to maximise the conditional distribution:
$$\textbf{y}_{prediction} = \argmax_{\textbf{y}} p(\textbf{y} | \textbf{x}) = \argmax_{\textbf{y}}\Bigg\{\prod_{t=1}^T p(y_t | y_{t-1})p(x_t | y_t)\Bigg\}$$
\end{itemize}
Take home message: we can (efficiently) predict the hidden sequence by maximising a likelihood
\end{frame}

%------------------------------------------------

\subsection{Logistic Regression}

%------------------------------------------------

\begin{frame}[noframenumbering]{Outline}
\tableofcontents[currentsubsection]
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Logistic Regression}
\begin{itemize}
\item A logistic regression is used for classifying a data sample into two (binary) or more (multi) categories, thus,
$$\hat{\text{y}}_{prediction} = \sigma\big(\boldsymbol\beta^{T} \cdot \boldsymbol{x}_{sample}\big),$$
where $\hat{y}$ is the prediction (represented as a probability), $\boldsymbol{x} = [x_0, x_1, ..., x_D]^T$ is a data sample, and $\boldsymbol\beta = [\beta_0, \beta_1, ..., \beta_D]^T$ is the vector of parameters we must \emph{learn}
\item We construct a (maximum log likelihood) cost function in terms of this parameter vector,
$$\mathcal{L}(\boldsymbol\beta) = \sum_{n=1}^N y_n\boldsymbol\beta^T\boldsymbol{x}_n - \log[1 + \exp(\boldsymbol\beta^T\boldsymbol{x}_n)]$$
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Solving a Logistic Regression}\begin{itemize}
\item Building a regression model is equivalent to solving a convex optimisation problem (i.e. maximising the cost function)
\item We know the form of the model, and we have a set of (training) data
\item We want to choose the model parameters for which the error is minimised (think line of best fit)
\item We use a numerical method to find the global minimum of error, for example, the method of gradient descent:
$$\boldsymbol\beta^{k+1} = \boldsymbol\beta^{k} - \alpha\nabla\mathcal{L}(\boldsymbol\beta^{k})$$
\end{itemize}
Take home message: we can automatically build mathematical functions for making predictions
\end{frame}

%------------------------------------------------

\subsection{Conditional Random Fields}

%------------------------------------------------

\begin{frame}[noframenumbering]{Outline}
\tableofcontents[currentsubsection]
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Conditional Random Fields (CRFs)}
\begin{itemize}
\item CRFs belong with HMMs to a broader class of models called \emph{graphical} models
\item Classifiers such as logistic regression only predict a single class; graphical models predict a full sequence
\item Because modelling the conditional probability, $p(\textbf{y} | \textbf{x})$ is sufficient for classification, a CRF avoids modelling the distribution on $\textbf{x}$, while allowing freedom over the choice of features (expressed as feature functions)
\item Whereas with a HMM we model the observation of word tokens, with a CRF, we can also model richer information about a token
\end{itemize}
\end{frame}

% ------------------------------------------------

\begin{frame}

\frametitle{Conditional Random Fields (CRFs)}
\begin{itemize}
\item A conditional random field factorises as,
$$p(\textbf{y}|\textbf{x}) = \frac{p(\textbf{x}, \textbf{y})}{\sum_{y'}{p(\textbf{x}, \textbf{y}')}},$$ where
$$
p(\textbf{x}, \textbf{y}) = \text{exp} \Bigg\{
\sum_{i \in S}
\sum_{j \in S}
\lambda_{ij}
F_{ij}(\textbf{y})
+ 
\sum_{i \in S}
\sum_{o \in O}
\mu_{io}
F_{io}(\textbf{x}, \textbf{y})
\Bigg\}
$$
$F_{io}(\textbf{x}, \textbf{y}) \approx \text{feature extraction}$
\end{itemize}

\end{frame}


%IT'S ALL VERY WELL FOR HMMS WHICH DROP THE P(X) ANYWAY WHEN IT COMES TO PREDICTION, BUT THIS IS NO GOOD FOR TRAINING A CLASSIFIER. AND SO WE DIVIDE THROUGH WITH THE P(X) AND FACTOR IT OUT IMPLICITLY BY THE LAW OF TOTAL PROBABILITIES. THIS IS THE TRICK OF CONDITIONAL RANDOM FIELDS.

% ------------------------------------------------

\begin{frame}
\frametitle{Conditional Random Fields (CRFs)}
\begin{itemize}
\item Like a logistic regression we have a cost function to minimise using greatest ascent techniques and solve for model parameters
\item There are some further tricks during training (inference algorithms)
\item And the optimisation algorithm is usually an approximation (l-BFGS)
\item But in the end, because the cost is dependent on the labels, we use the Viterbi algorithm to do predictions, just like HMMs.
\end{itemize}
Take home message: CRFs are very similar to HMMs; we predict in the same way; but we learn the parameters through training like logistic regression; and modelling the conditional distribution gives us the freedom of exploiting richer features.
\end{frame}

% ------------------------------------------------

\section{Grobid}
\begin{frame}[noframenumbering]{Outline}
\tableofcontents[currentsection]
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Grobid}
\begin{itemize}
\item Grobid (GeneRation Of BIbliographic Data) is a Java-based tool for managing CRF models 
\item It coordinates the training and usage of a ``cascade'' of models, computed with a CRF engine backend
\item CROSSREF
\end{itemize}
\begin{figure}[!ht]
\center
\includegraphics[width=4in]{figures/cascade.pdf}
\caption{Cascade of models used by Grobid}
\end{figure}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Grobid - Details}
TEI example
\end{frame}

%------------------------------------------------

\section{Initial Results}
\begin{frame}[noframenumbering]{Outline}
\tableofcontents[currentsection]
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Initial Results}
TABLE HERE
\end{frame}

%------------------------------------------------

\section{Next Steps}
\begin{frame}[noframenumbering]{Outline}
\tableofcontents[currentsection]
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Next Steps}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Project Objectives}
\begin{itemize}
\item There are some special cases we want to deal with in HEP papers
\item Take the existing state-of-the-art in metadata extraction and optimise it for HEP papers
\item We will use Conditional Random Fields (CRFs) to build a probabilistic model
\item CRFs combine ideas from log linear models (logistic regressions) and hidden Markov models (HMMs)
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{References}

\begin{thebibliography}{0}

\bibitem{comparison} LIPINSKI, M., YAO, K., BREITINGER, C., BEEL, J., AND GIPP, B. Evaluation of header metadata extraction approaches and tools for scientific PDF documents. In JCDL (2013), pp. 385-386.

\end{thebibliography}

\end{frame}

\end{document} 